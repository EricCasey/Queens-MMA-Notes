# NEAT TERMINOLOGY

## Active Inertia / Cognitive Inertia
    * When you set a belief and it fuck you over later

## _A Priori_
    * 'from the earlier'

## _A Posteriori_
    * 'from the later'

## _Non Sequitur_
    * 'it does not follow'

## _Likelyhood =/= Probablity_
    * 'in stats, likelyhood and probablity are not synonymous'

## Type I Error
    * H<sub>0</sub> is true but regected.

## Type II Error
    * H<sub>0</sub> is false but accepted.

## Null Hypothesis (H<sub>0</sub>)
    * The equality bound always goes into the null hypothesis. i.e. the >= & =< are always in H<sub>0</sub>
    * The "not interesting" result

## Pedantic
    * ostentatious in one's learning.

## Meta-Conversation
    * Talking about your conversation about something else

## Heteroskedasticity
    * hetero “different” and skedasis “dispersion”

## Data Deluge or Information Explosion
    * the side effects of bringing lots of information into scope

## Dagwood Sandwich Problem
    * Multi layered multi parameter functions, like callback hell.

## Gaussian or Normal Curve
    * You know the one.

## Preattentive Processing
    * Spidey Sense (<250ms)

## Visualization: 'Pop'
    * The speed at which you can decode a vis

## C.R.A.P. of Good Design
    * Contrast, Alignment, Repetition, Proximity

## Hicks Law    
    * More Choice, More Time. Rate of gain of Information

## Chart Junk
    * Anyhting included in a chart that's junk obviously

## Data / Ink Ration
    * Amount of text vs data

## UX "Affordances"
    * Switches, buttons, things a user must do

## UX "Signifier"
    * Showing users what the controls are

## UX "Mapping"
    * Working with how the user expects controls to work

## The 7 Types Of Lean Manufacturing Waste (TIMWOOD)
    * Transport
    * Inventory
    * Motion
    * Waiting
    * Over Processing
    * Over Production
    * Defects

## EDA: Exploratory Data Analysis
    * 

## Brownian Motion
    * Simulation of random motion of particles in a fluid or gas.

## EMV: Expected Monetary Value
    * The number used to compare different decisions.

## Anscombe's Quartet
    * Four scatterplots that look very different but have the same outputs

## Least Squares Regression Line
    * The line where the sum of the distrance from each point squared is the minimum

## Generic Statistical Model
    * response = f(explanatory) + noise
    * Y => X + e

## Utility Function
    * The imaginary function of a customer's perspective on the usefullness of a product to them.

## Nash Equilibrium
    * Is a solution concept of a non-cooperative game involving two or more players in which each player is assumed to know the equilibrium strategies of the other players, and no player has anything to gain by changing only their own strategy.

## Confusion Matrix
    * this is the rate at which type I & type II errors are made.

## Receiver Opererating Curve (ROC CURVE)
    * Developed during WWII for Radar.

## Area Under Curve (AUC)
    * Pretty fucking self fucking explanatory no
    * Perfect model is 1, average for a random model is 0.5
    * Think of it as a letter grade, happy with 0.8, 0,7 isn't bad.

## GLMNET Models: Generalized Linear Models
    * Attempts to find a parsimonious model. Helps with colieearity, also better with small sample sizes. Pairs well with random forest models. Uses balance of Lasso and Ridge regressions.
    * Turing Parameters:
        * Alpha: Mix of Lasso : Ridge Regressions
        * Lamda: Strength of the penalty on the coefficients
    * Lasso Regression
        * Peanlizes the number of non-zero coefficients
    * Ridge Regression
        * Penalizes the absolute magnitude of the coefficients

## Parsimonious
    * Frugal, unwilling to spen money or use resources

## KNN Inputation
    * K Nearest Neighbours Imputation, tryies to make it up based on existing data points like it

## Principle Compoent Analysis
    * Combines the low variance and correlated variables into a single set of hifh-variance, perpendicular predictors.
    * 

## The 5 Vs of Big Data (only 3 are legit)
    * Volume - The size of it all
    * Variety - The range of sources
    * Velocity - The speed at which it's growing
    * Veracity (bullshit)
    * Value (bullshit)

## Edge Computing
    * Keeps only a rolling state rather than storing it all

## What Counts As Big Data?
    * You can't hold it or analyze it on an average consumer-level computer.

## What makes Data Smart?
    * Data that provides values to the bottom line somehow

## RMSE: Root Mean Squared Error
    * The measurement of the closeness of the model to new values.

## Aritificial Intelligence 
    * AI : Artifical Intelligence
        * ML : Machine Learning --> Algorithms that improve over time through exposure to more data
            * DL : Deep Learning --> Subset of Machine Learning that uses advanced Neural Networks
                - Convolution NN : 
                - Recursive NN : 

## Supervised Learning
    * Uses training data and feedback from humans to learn relationships.
    * Regression, Classification
    * When you want to make predictions on labelled data

## Unsupervised Learning
    * an algorithm explores input data without being diven an explicit output variable.
    * Clustering, Anomalies
    * Finding structure in unlabeled data
    * Dimensionality Reduction (reducing the number of dimensions effectively) (makes it simpler)
        * Principal Cmponent Analysis
            * Finds a linear combination of variables to create principle components
            * principle components should be uncorrelated
    * Used for PreProcessing before Supervised Learning
    * Requires creativity

## K-means Clustering Algorithm
    * Base install in R, Basically Breaks overservations into pre-defined number of clusters
    * 

## Hierarchical Clustering
    * Top Down
        * 
    * Bottom Up
        * 

# Linking Cluster Methods:
    * Complete *good
        * pairwise similarity between all obsercations in cluster 1 and cluster 2, and uses largest of similarities
    * Single *not great
        * same as above but uses smalles of similarities
    * Average *good
        * same as above but uses average
    * Centroid *dangerous
        * finds centroid of cluster 1 and centrind of cluster 2, and uses similaritiy between two centroids.

## Dentrogram
    * Tree shaped structure to interperet hierarchical clustering models

## Reinforcement Learning
    * An algorithm learns to perform a task simply by trying to maximize rewards it receiveds for its actions.
    * Games (Chess, Go, Poker, Starcraft)
    * Works when clear "rules" are present and machines can play with themselves (create its own data to train itself)
    * Learns from operating in a real environment

## Moore's Law
    * Computing power doubles every two years.

## Back Propogation
    * Helps calculate the weight of results from a Neural Network

## ARIMA Models
    * 

## Text Corpus
    * Collection of documents or tesxt

## Little's Law
    * 

## Gantt Chart
    * Timeline of stacked process steps or walk-ins or whatever

## Antithetic
    * Against the thesis, 'antithesis'

## FUN FACTS:
    * AI Translation French-English is most robust because of Canada's public parliamentary notes